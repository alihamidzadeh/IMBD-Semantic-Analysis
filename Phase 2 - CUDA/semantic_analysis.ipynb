{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis on GPU using cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Set\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Print the name of the GPU\n",
    "print(cuda.Device(0).name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Preprocessing on CPU\n",
    "1. load the csv dataset\n",
    "2. clean the text\n",
    "3. tokenize the text\n",
    "4. buliding the vocabulary with this format {word:index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV and return list of dicts or DataFrame\n",
    "def load_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file with columns:\n",
    "    - review: text of the review\n",
    "    - sentiment: positive/negative\n",
    "    Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    # Remove empty reviews if any\n",
    "    df = df.dropna(subset=[\"review\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text: lowercase, remove HTML tags, numbers, punctuation\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()                       # Convert to lowercase \n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)      # Remove HTML tags\n",
    "    text = re.sub(r\"\\d+\", \" \", text)          # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)      # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text: split by space\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary: all unique words across all reviews, minus stopwords\n",
    "def build_vocabulary(reviews: List[str], stopwords: Set[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    reviews: list of cleaned review texts\n",
    "    stopwords: set of words to ignore\n",
    "    Returns: {word: index}\n",
    "    \"\"\"\n",
    "    vocab_set = set()\n",
    "    for review in reviews:\n",
    "        tokens = tokenize(review)\n",
    "        for word in tokens:\n",
    "            if len(word) >= 2 and word not in stopwords:\n",
    "                vocab_set.add(word)\n",
    "    # Assign index to each word\n",
    "    vocab = {word: idx for idx, word in enumerate(sorted(vocab_set))}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transform Text into Numeric Data on CPU\n",
    "1. preprocess dataset using step 1 funtions\n",
    "2. convert text into numeric arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a review to a numeric array \n",
    "def reviews_to_token_indices(\n",
    "    reviews: List[str],\n",
    "    vocab: Dict[str, int]\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert each cleaned review to an array of token indices.\n",
    "    \"\"\"\n",
    "    token_arrays = []\n",
    "    for text in reviews:\n",
    "        tokens = tokenize(text)\n",
    "        indices = [vocab[word] for word in tokens if word in vocab]\n",
    "        token_arrays.append(np.array(indices, dtype=np.int32))\n",
    "    return token_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV dataset\n",
    "df = load_csv('./Minimal_IMDB_Dataset.csv')\n",
    "\n",
    "# Clean all reviews\n",
    "df[\"cleaned\"] = df[\"review\"].apply(clean_text)\n",
    "\n",
    "# Prepare your stopwords set\n",
    "stopwords = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "    \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
    "    \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "    \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n",
    "    \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\",\n",
    "    \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n",
    "    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\",\n",
    "    \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\",\n",
    "    \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "}\n",
    "\n",
    "# Build the vocab\n",
    "vocab = build_vocabulary(df[\"cleaned\"].tolist(), stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to 0/1\n",
    "y = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0).to_numpy()\n",
    "\n",
    "# Convert cleaned reviews to token index arrays\n",
    "X_token_indices = reviews_to_token_indices(df[\"cleaned\"].tolist(), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(X_token_indices)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute TF (Term Frequencies) on GPU\n",
    "1. CUDA kernel to count token frequencies.\n",
    "2. Allocate GPU memory\n",
    "3. Transfer token arrays to GPU\n",
    "4. Launche the kernel\n",
    "5. Copies results back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a CUDA kernel to count token frequencies\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void compute_tf(int *tokens, int *counts, int num_tokens, int vocab_size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= num_tokens)\n",
    "        return;\n",
    "\n",
    "    int token = tokens[idx];\n",
    "    if (token >= 0 && token < vocab_size) {\n",
    "        atomicAdd(&counts[token], 1);\n",
    "    }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens: np.ndarray, vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute term frequencies for one review.\n",
    "    tokens: array of token indices (int32)\n",
    "    vocab_size: size of vocab\n",
    "    Returns: frequency array (int32)\n",
    "    \"\"\"\n",
    "    # Prepare output array\n",
    "    counts_host = np.zeros(vocab_size, dtype=np.int32)\n",
    "\n",
    "    # Allocate GPU memory\n",
    "    tokens_gpu = cuda.mem_alloc(tokens.nbytes)\n",
    "    counts_gpu = cuda.mem_alloc(counts_host.nbytes)\n",
    "\n",
    "    # Copy tokens to GPU\n",
    "    cuda.memcpy_htod(tokens_gpu, tokens)\n",
    "    cuda.memcpy_htod(counts_gpu, counts_host)\n",
    "\n",
    "    # Kernel function\n",
    "    kernel = mod.get_function(\"compute_tf\")\n",
    "\n",
    "    # Launch kernel\n",
    "    block_size = 256\n",
    "    grid_size = int((tokens.size + block_size - 1) / block_size)\n",
    "\n",
    "    kernel(\n",
    "        tokens_gpu,\n",
    "        counts_gpu,\n",
    "        np.int32(tokens.size),\n",
    "        np.int32(vocab_size),\n",
    "        block=(block_size,1,1),\n",
    "        grid=(grid_size,1)\n",
    "    )\n",
    "\n",
    "    # Copy results back\n",
    "    cuda.memcpy_dtoh(counts_host, counts_gpu)\n",
    "\n",
    "    return counts_host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectors = []\n",
    "for tokens in X_token_indices:\n",
    "    tf = compute_tf(tokens, vocab_size=len(vocab))\n",
    "    tf_vectors.append(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute IDF (Inverse Document Frequencies) on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute TF-IDF Vectors on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Logistic Regression Model on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ocr_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
